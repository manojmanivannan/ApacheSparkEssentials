{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Custom ML transformers & Estimators\n",
    "\n",
    "We will see how to create and use custom transformers and estimators in this chapter. Because of how similar transformers and estimators are, we start with in-depth coverage of the transformer and its fundamental building block, the Param. We will also see how to integrate custom transformers and estimators in an ML pipeline.\n",
    "\n",
    "### Creating your own transformer\n",
    "\n",
    "We implement a `ScalarNAFiller` transformer that fills the `null` values of a column with a scalar value instead of the `mean` or `median` when using the `Imputer`. Thanks to this, our dessert pipeline from chapter 13 will have a `ScalarNAFiller` stage that we’ll be able to use when running different scenarios—when optimizing hyperparameters, for instance — without changing the code itself. This improves the flexibility and robustness of our ML experiments.\n",
    "\n",
    "Our blueprint for this section follows this plan:\n",
    "1. Design our transformer: Params, inputs, and outputs.\n",
    "2. Create the Params, inheriting some preconfigured ones as necessary.\n",
    "3. Create the necessary getters and setters to get.\n",
    "4. Create the initialization function to instantiate our transformer.\n",
    "5. Create the transformation function.\n",
    "\n",
    "The PySpark Transformer class (`pyspark.ml.Transformer`) provides many of the methods we used in chapter 13, such as `explainParams()` and `copy()`, plus a handful of other methods that will prove useful for implementing our own transformers. By sub-classing `Transformer`, we inherit all of this functionality for free, like we do in the following listing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "\n",
    "class ScalarNAFiller(Transformer):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/custom_scalarna_filter.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Designing a transformer: Thinking in terms of Params and transformation\n",
    "\n",
    "In chapters 12 and 13, we saw that a transformer (and, by extension, an estimator) is configured through a collection of Params. The `transform()` function always takes a data frame as an input and returns a transformed data frame. We want to stay consistent with our design to avoid problems at use time.\n",
    "\n",
    "When designing a custom transformer, we always start by implementing a function that reproduces the behavior of our transformer. For the `ScalarNAFiller`, we leverage the `fillna()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----+----+\n",
      "|one|two|three|four|five|\n",
      "+---+---+-----+----+----+\n",
      "|  1|  2|    4|   1|   4|\n",
      "|  3|  6|    5|   4|   5|\n",
      "|  9|  4| null|   9| -99|\n",
      "| 11| 17| null|   3| -99|\n",
      "+---+---+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Column, DataFrame\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Custom Transformers\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "test_df = spark.createDataFrame(\n",
    "    [\n",
    "        [1, 2, 4, 1], \n",
    "        [3, 6, 5, 4], \n",
    "        [9, 4, None, 9], \n",
    "        [11, 17, None, 3]\n",
    "    ],\n",
    "    [\"one\", \"two\", \"three\", \"four\"],\n",
    ")\n",
    "\n",
    "def scalarNAFillerFunction(df: DataFrame, inputCol: Column, outputCol: str, filler: float = 0.0):\n",
    "    return df.withColumn(outputCol, inputCol).fillna(\n",
    "        filler, subset=outputCol\n",
    "    )\n",
    "\n",
    "scalarNAFillerFunction(test_df, F.col(\"three\"), \"five\", -99.0).show()\n",
    "# null in column 'three' has been replace by -99, our filler value\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we immediately see that we need three Params in our `ScalarNAFiller`:\n",
    "- `inputCol` and `outputCol` are for the input and output columns, following the same behavior as the other transformers and estimators we’ve encountered thus far.\n",
    "- `filler` contains a floating-point number for the value that `null` will be replaced with during the `transform()` method.\n",
    "\n",
    "The data frame (df in code above) would get passed as an argument to the `transform()` method. Should we want to map this into the transformer blueprint introduced in chapter 13, it would look like figure below\n",
    "\n",
    "<img src=\"images/scalarna_filler_blueprint.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Params of a transformer\n",
    "\n",
    "In this section, we create the three Params (`inputCol`, `outputCol`, `filler`) for the `ScalarNAFiller` transformer. We learn how to define a Param from scratch that will play well with other Params. Params drive the behavior of the transformer and estimator, and allow for easy customization when running a pipeline.\n",
    "\n",
    "First, we start with the creation of a custom Param, our filling value filler. To create a custom Param, PySpark provides a Param class with four attributes:\n",
    "- A `parent`, which carries the value of the transformer once the transformer is instantiated.\n",
    "- A `name`, which is the name of our Param. By convention, we set it to the same name as our Param.\n",
    "- A `doc`, which is the documentation of our Param. This allows us to embed documentation for our Param when the transformer will be used.\n",
    "- A `typeConverter`, which governs the type of the Param. This provides a standardized way to convert an input value to the right type. It also gives a relevant error message if, for example, you expect a floating-point number, but the user of the transformer provides a string.\n",
    "\n",
    "Every custom Param we create needs to have `Params._dummy()` as a parent; this ensures that PySpark will be able to copy and change transformers' Params when you use or change them, for instance, during cross-validation (chapter 13). The name and doc are self-explanatory, so let’s spend a little more time on the `typeConverter`. \n",
    "\n",
    "Type converters are the way we instruct the Param about the type of value it should expect. Think of them like value annotations in Python, but with the option to try to convert the value. In the case of the filler, we want a floating-point number, so we use `TypeConverters.toFloat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='undefined', name='filler', doc='Value we want to replace our null values with.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.param import Param, Params, TypeConverters\n",
    "\n",
    "filler = Param(\n",
    "    Params._dummy(),\n",
    "    \"filler\",\n",
    "    \"Value we want to replace our null values with.\",\n",
    "    typeConverter=TypeConverters.toFloat,\n",
    ")\n",
    "\n",
    "filler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly used Params are defined in special classes called Mixin under the `pyspark.ml.param.shared` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HasInputCols(Params):\n",
    "    \"\"\"Mixin for param inputCols: input column names.\"\"\"\n",
    "    inputCols = Param(\n",
    "        Params._dummy(),\n",
    "        \"inputCols\", \"input column names.\",\n",
    "        typeConverter=TypeConverters.toListString,\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasInputCols, self).__init__()\n",
    "\n",
    "    def getInputCols(self):\n",
    "        \"\"\"Gets the value of inputCols or its default value. \"\"\"\n",
    "        return self.getOrDefault(self.inputCols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use these accelerated Param definitions, we simply have to sub-class them in our transformer class definition. Our updated class definition now has all three Params defined: two of them through a Mixin (`inputCol`, `outputCol`), and one custom (`filler`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "\n",
    "class ScalarNAFiller(Transformer, HasInputCol, HasOutputCol):\n",
    "    filler = Param(\n",
    "        Params._dummy(),\n",
    "        \"filler\",\n",
    "        \"Value we want to replace our null values with.\",\n",
    "        typeConverter=TypeConverters.toFloat,\n",
    "    )\n",
    "\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getters and setters: Being a nice PySpark citizen\n",
    "\n",
    "Based on the design of every PySpark transformer we have used so far, the simplest way to create setters is as follows: we first create a general method, `setParams()`, that allows us to change multiple parameters passed as keyword arguments. Then, creating the setter for any other Param will simply call `setParams()` with the relevant keyword argument. The `setParams()` method is difficult to get right at first; it needs to accept any Params our transformer has and then update only those we are passing as arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "\n",
    "@keyword_only\n",
    "def setParams(self, *, inputCol=None, outputCol=None, filler=None):\n",
    "    kwargs = self._input_kwargs\n",
    "    return self._set(**kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `setParams()` cleared out, it’s time to create the individual setters. That couldn’t be easier: simply call `setParams()` with the appropriate argument! In previously seen code, we saw that, while the getter for inputCol is provided, the setter is not because it would imply creating a generic `setParams()` that we’d override anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setFiller(self, new_filler):\n",
    "    return self.setParams(filler=new_filler)\n",
    "\n",
    "def setInputCol(self, new_inputCol):\n",
    "    return self.setParams(inputCol=new_inputCol)\n",
    "\n",
    "def setOutputCol(self, new_outputCol):\n",
    "    return self.setParams(outputCol=new_outputCol)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setters are done! Now it’s time for the getters. Unlike setters, getters for Mixin are already provided, so we only have to create `getFiller()`. We also do not have to create a generic `getParams()`, since the `Transformer` class provides `explainParam` and `explainParams` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFiller(self):\n",
    "    return self.getOrDefault(self.filler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalarNAFiller(Transformer, HasInputCol, HasOutputCol):\n",
    "    filler = [...] # elided for terseness\n",
    "    \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, filler=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "        \n",
    "    def setFiller(self, new_filler):\n",
    "        return self.setParams(filler=new_filler)\n",
    "    \n",
    "    def getFiller(self):\n",
    "        return self.getOrDefault(self.filler)\n",
    "    \n",
    "    def setInputCol(self, new_inputCol):\n",
    "        return self.setParams(inputCol=new_inputCol)\n",
    "    \n",
    "    def setOutputCol(self, new_outputCol):\n",
    "        return self.setParams(outputCol=new_outputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05de79a9bc4beb95fb2b07d395d8e3fe55e6d8497bda19361fbfb16b724883dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
