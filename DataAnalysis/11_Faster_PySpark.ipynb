{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster PySpark: Understanding Spark's query planning\n",
    "\n",
    "Imagine the following scenario: you write a readable, well-thought-out PySpark program. When submitting your program to your Spark cluster, it runs. You wait. \n",
    "\n",
    "How can we peek under the hood and see the progression of our program?  Troubleshoot which step is taking a lot of time? This chapter is about understanding how we can access information about our Spark instance, such as its configuration and layout (CPU, memory, etc.). We also follow the execution of a program from raw Python code to optimized Spark instructions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating the Spark UI to understand the environment\n",
    "\n",
    "This section covers how Spark uses allocated computing and memory resources and how we can configure how many resources are assigned to Spark. \n",
    "\n",
    "Our program follows a pretty simple set of steps:\n",
    "1. We create a SparkSession object to access the data frame functionality of PySpark as well as to connect to our Spark instance.\n",
    "2. We create a data frame containing all the text files (line by line) within the chosen directory, and we count the occurrence of each word.\n",
    "3. We show the top 10 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Counting word occurrences from a book, one more time.</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f958c7e580>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Counting word occurrences from a book, one more time.\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the|39188|\n",
      "| and|24292|\n",
      "|  of|21234|\n",
      "|  to|20581|\n",
      "|   i|15151|\n",
      "|   a|14564|\n",
      "|  in|12857|\n",
      "|that| 9900|\n",
      "|  it| 9451|\n",
      "| was| 8939|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = (\n",
    "    spark.read.text(\"./data/gutenberg_books/*.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']+\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(F.col(\"word\"))\n",
    "    .count()\n",
    ")\n",
    "results.orderBy(F.col(\"count\").desc()).show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to the **Spark UI**. The Spark UI landing pages (also known as the Job tab on the top menu) contain a lot of information, which we can divide into a few sections:\n",
    "- The top menu provides access to the main sections of the Spark UI, which we explore in this chapter.\n",
    "- The timeline provides a visual overview of the activities impacting your SparkSession; in our case, we see the cluster allocating resources (an executor driver, since we work locally) and performing our program.\n",
    "- The jobs, which in our case are triggered by the `show()` action (depicted in the Spark UI as `showString`), are listed at the bottom of the page. In the case where a job is processing, it would be listed as in progress"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviewing the configuration: The environment tab\n",
    "This tab contains the configuration of the environment that our Spark instance sits on, so the information is useful for troubleshooting library problems, providing configuration information if you run into weird behavior (or a bug!), or understanding the specific behavior of a Spark instance.\n",
    "\n",
    "The Environment tab contains all the information about how the machines on your cluster are set up. It covers information about the JVM and Scala versions installed (remember, Spark is a Scala program), as well as the options Spark is using for this session. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greater than the sum of its parts; The Executors tab and resource management\n",
    "\n",
    "Executors tab contains information about the computing and memory resources available to our Spark instance. After clicking on Executors, we are presented with a summary and detailed view of all the nodes in our cluster. \n",
    "CPU cores and RAM used.  By default, Spark will allocate 1 GiB (gebibyte) of memory to the driver process. \n",
    "\n",
    "Spark uses RAM for three main purposes :\n",
    "- A portion of the RAM is reserved for Spark internal processing, such as user data structures, internal metadata, and safeguarding against potential out-of-memory errors when dealing with large records.\n",
    "- The second portion of the RAM is used for operations (operational memory). This is the RAM used during data transformation.\n",
    "- The last portion of the RAM is used for the storage (storage memory) of data. RAM access is a lot faster than reading and writing data from and to disk, so Spark will try to put as much data in memory as possible. If operational memory needs grow beyond whatâ€™s available, Spark will spill some of the data from RAM to disk.\n",
    "\n",
    "<img src=\"images/spark_resources.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides a few configuration flags to change the memory and number of CPU cores available. We have access to two identical sets of parameters to define the resources our drivers and executors will have access to. \n",
    "\n",
    "When creating the `SparkSession`, you can set the `master()` method to connect to\n",
    "a specific cluster manager (in cluster mode) when working locally and specify the resources/number of cores to allocate from your computer.\n",
    "\n",
    "We can decide to go from 16 cores to only 8 by passing `master(\"local[8]\")` in the `SparkSession` builder object. Memory allocation is done through configuration flags; the most important when working locally is `spark.driver.memory`. This flag takes size as an attribute and is set via the `config()` method of the SparkSession builder object.\n",
    "\n",
    "|Abbreviation| Definition  |\n",
    "|------------|-------------|\n",
    "|1b        |    1 byte|\n",
    "|1k or 1kb |1 kibibyte = 1,024 bytes|\n",
    "|1m or 1mb |1 mebibyte = 1,024 kibibytes|\n",
    "|1g or 1gb |1 gibibyte = 1,024 mebibytes|\n",
    "|1t or 1tb |1 tebibyte = 1,024 gibibytes|\n",
    "|1p or 1pb |1 pebibyte = 1,024 tebibytes|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Launching PySpark with custom options\")\n",
    "    .master(\"local[8]\")\n",
    "    .config(\"spark.driver.memory\", \"16g\")\n",
    ").getOrCreate()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# REFER CHAPTER 11 of the book\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05de79a9bc4beb95fb2b07d395d8e3fe55e6d8497bda19361fbfb16b724883dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
