{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bilingual PySpark: Blending Python & SQL code\n",
    "\n",
    "In this section, we'll see how we can use python and SQL together with PySpark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import from libs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing  pyspark.sql vs. plain SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|period|count|\n",
      "+------+-----+\n",
      "|     6|    1|\n",
      "|     4|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elements = spark.read.csv(\n",
    "    \"data/elements/Periodic_Table_Of_Elements.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "elements.where(F.col(\"phase\") == \"liq\").groupby(\"period\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT\n",
    "    period,\n",
    "    count(*)\n",
    "FROM \n",
    "    elements\n",
    "WHERE \n",
    "    phase = 'liq'\n",
    "GROUP BY \n",
    "    period;\n",
    "```\n",
    "\n",
    "<img src=\"images/python_sql_comparison.png\" height=\"300px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how to get spark data frame using SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table or view not found: elements; line 1 pos 29;\n",
      "'Aggregate ['period], ['period, unresolvedalias(count(1), None)]\n",
      "+- 'Filter ('phase = liq)\n",
      "   +- 'UnresolvedRelation [elements], [], false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql(\n",
    "        \"select period, count(*) from elements \"\n",
    "        \"where phase='liq' group by period\"\n",
    "    ).show(5)\n",
    "except AnalysisException as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, PySpark doesn’t make the link between the python variable elements, which points to the data frame, and a potential table elements that can be queried by Spark SQL. To allow a data frame to be queried via SQL, we need to **register** it.\n",
    "\n",
    "When you want to create a table/view to query with Spark SQL, use the createOrReplaceTempView() method. This method takes a single string parameter, which is the name of the table you want to use. This transformation will look at the data frame referenced by the Python variable on which the method was applied and will create a Spark SQL reference to the same data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|period|count(1)|\n",
      "+------+--------+\n",
      "|     6|       1|\n",
      "|     4|       1|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elements.createOrReplaceTempView(\"elements\")\n",
    "\n",
    "spark.sql(\n",
    "    \"select period, count(*) from elements where phase='liq' group by period\"\n",
    ").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how to manage these registered views/tables. Spark has a way of managing it via the **Catalog**. PySpark has four methods to create temporary views, and they look quite similar at first glance:\n",
    "- `createGlobalTempView()`\n",
    "- `createOrReplaceGlobalTempView()`\n",
    "- `createOrReplaceTempView()`\n",
    "- `createTempView()`\n",
    "\n",
    "The Spark catalog is an object that allows working with Spark SQL tables and views. A lot of its methods have to do with managing the metadata of those tables, such as their names and the level of caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.catalog.Catalog at 0x16b5e1d5b80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='elements', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropTempView(\"elements\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL and PySpark\n",
    "\n",
    "data downloaded from [here](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q3_2019.zip). To know more about the dataset check this [link](http://mng.bz/4jZa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"data/backblaze/\"\n",
    "\n",
    "backblaze_2019 = spark.read.csv(\n",
    "    DIRECTORY+\"*.csv\", \n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the layout for each column according to the schema\n",
    "backblaze_2019 = backblaze_2019.select(\n",
    "    [\n",
    "        F.col(x).cast(T.LongType()) if x.startswith(\"smart\") else F.col(x)\n",
    "        for x in backblaze_2019.columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "backblaze_2019.createOrReplaceTempView(\"backblaze_stats_2019\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: to perform a quick exploratory data analysis on a subset of the columns presented. Then will reproduce the failure rates that Backblaze computes and identify the models with the greatest and least amount of failures in 2019."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the rows and columns you want: select and where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|serial_number|\n",
      "+-------------+\n",
      "|     ZA10MCJ5|\n",
      "|     ZCH07T9K|\n",
      "|     ZCH0CA7Z|\n",
      "|     Z302F381|\n",
      "|     ZCH0B3Z2|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"select serial_number from backblaze_stats_2019 where failure = 1\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|serial_number|\n",
      "+-------------+\n",
      "|     ZA10MCJ5|\n",
      "|     ZCH07T9K|\n",
      "|     ZCH0CA7Z|\n",
      "|     Z302F381|\n",
      "|     ZCH0B3Z2|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "backblaze_2019.where(\"failure = 1\").select(F.col(\"serial_number\")).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping similar records together: group by and order by\n",
    "\n",
    "Let's start looking at the capacity, in gigabytes, of the hard drives included in the data, by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               model|              min_GB| max_GB|\n",
      "+--------------------+--------------------+-------+\n",
      "| TOSHIBA MG07ACA14TA|             13039.0|13039.0|\n",
      "|       ST12000NM0117|             11176.0|11176.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALE600|             11176.0|11176.0|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"SELECT\n",
    "        model,\n",
    "        min(capacity_bytes / pow(1024, 3)) min_GB,\n",
    "        max(capacity_bytes/ pow(1024, 3)) max_GB\n",
    "    FROM backblaze_stats_2019\n",
    "    GROUP BY 1\n",
    "    ORDER BY 3 DESC\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               model|              min_GB| max_GB|\n",
      "+--------------------+--------------------+-------+\n",
      "| TOSHIBA MG07ACA14TA|             13039.0|13039.0|\n",
      "|       ST12000NM0117|             11176.0|11176.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALE600|             11176.0|11176.0|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "backblaze_2019.groupby(F.col(\"model\")).agg(\n",
    "    F.min(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"min_GB\"),\n",
    "    F.max(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"max_GB\"),\n",
    ").orderBy(\n",
    "    F.col(\"max_GB\"), ascending=False\n",
    "    ).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering after grouping using having\n",
    "\n",
    "Looking at the results from our query, there are some drives that report more than one capacity. Furthermore, we have some drives that report negative capacity, which is really odd. Let’s focus on seeing how prevalent this is.\n",
    "\n",
    "Because of the order of the evaluation of operations in SQL, where is always applied before group by. What happens if we want to filter the values of columns created after the group by operation? We use a new keyword: `having`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+\n",
      "|               model|              min_GB|           max_GB|\n",
      "+--------------------+--------------------+-----------------+\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|          11176.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|          11176.0|\n",
      "|HGST HUH721010ALE600|-9.31322574615478...|           9314.0|\n",
      "|       ST10000NM0086|-9.31322574615478...|           9314.0|\n",
      "|        ST8000NM0055|-9.31322574615478...|7452.036460876465|\n",
      "+--------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"SELECT\n",
    "        model,\n",
    "        min(capacity_bytes / pow(1024, 3)) min_GB,\n",
    "        max(capacity_bytes/ pow(1024, 3)) max_GB\n",
    "    FROM backblaze_stats_2019\n",
    "    GROUP BY 1\n",
    "    HAVING min_GB != max_GB\n",
    "    ORDER BY 3 DESC\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+\n",
      "|               model|              min_GB|           max_GB|\n",
      "+--------------------+--------------------+-----------------+\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|          11176.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|          11176.0|\n",
      "|HGST HUH721010ALE600|-9.31322574615478...|           9314.0|\n",
      "|       ST10000NM0086|-9.31322574615478...|           9314.0|\n",
      "|        ST8000NM0055|-9.31322574615478...|7452.036460876465|\n",
      "+--------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "backblaze_2019.groupby(F.col(\"model\")).agg(\n",
    "    F.min(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"min_GB\"),\n",
    "    F.max(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"max_GB\"),\n",
    ").where(F.col(\"min_GB\") != F.col(\"max_GB\")).orderBy(\n",
    "    F.col(\"max_GB\"), ascending=False\n",
    ").show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s materialize our work, SQL-style\n",
    "#### Creating new tables/views using the CREATE keyword\n",
    "\n",
    "Creating a table or a view is very easy in SQL: prefix our query by CREATE TABLE/VIEW. \n",
    "Let's reproduce the drive_days and failures that compute the number of days of operation that a model has and the number of drive failures it has had, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "backblaze_2019.createOrReplaceTempView(\"drive_stats\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW drive_days AS\n",
    "        SELECT model, count(*) AS drive_days\n",
    "        FROM drive_stats\n",
    "        GROUP BY model\"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"CREATE OR REPLACE TEMP VIEW failures AS\n",
    "        SELECT model, count(*) AS failures\n",
    "        FROM drive_stats\n",
    "        WHERE failure = 1\n",
    "        GROUP BY model\"\"\"\n",
    ")\n",
    "\n",
    "drive_days = backblaze_2019.groupby(F.col(\"model\")).agg(\n",
    "    F.count(F.col(\"*\")).alias(\"drive_days\")\n",
    ")\n",
    "\n",
    "failures = (\n",
    "    backblaze_2019.where(F.col(\"failure\") == 1)\n",
    "    .groupby(F.col(\"model\"))\n",
    "    .agg(F.count(F.col(\"*\")).alias(\"failures\"))\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: ***Creating tables from data in SQL***: You can also create a table from data on a hard drive or HDFS. For this, you can use a modified SQL query. Since we are reading a CSV file, we prefix our path with csv.: ``spark.sql(\"create table q1 as select * from csv.`./data/backblaze/drive_stats_2019_Q1`\")``"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding data to our table using UNION and JOIN\n",
    "\n",
    "Joins and unions are the only clauses we’ll see that modify the target piece in our SQL statement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "columns_backblaze = \", \".join(q4.columns)\n",
    "\n",
    "q1.createOrReplaceTempView(\"Q1\")\n",
    "q2.createOrReplaceTempView(\"Q2\")\n",
    "q3.createOrReplaceTempView(\"Q3\")\n",
    "q4.createOrReplaceTempView(\"Q4\")\n",
    "\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW backblaze_2019 AS\n",
    "    SELECT {col} FROM Q1 UNION ALL\n",
    "    SELECT {col} FROM Q2 UNION ALL\n",
    "    SELECT {col} FROM Q3 UNION ALL\n",
    "    SELECT {col} FROM Q4\n",
    "\"\"\".format(\n",
    "    col=columns_backblaze\n",
    "    )\n",
    ")\n",
    "\n",
    "backblaze_2019 = (\n",
    "    q1.select(q4.columns)\n",
    "    .union(q2.select(q4.columns))\n",
    "    .union(q3.select(q4.columns))\n",
    "    .union(q4)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+\n",
      "|        model|drive_days|failures|\n",
      "+-------------+----------+--------+\n",
      "|  ST9250315AS|        89|    null|\n",
      "|  ST4000DM000|   1796728|      72|\n",
      "|ST12000NM0007|   3212635|     364|\n",
      "|  ST8000DM005|      2280|       1|\n",
      "|   ST320LT007|        89|    null|\n",
      "+-------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining our tables\n",
    "spark.sql(\n",
    "    \"\"\"select\n",
    "            drive_days.model,\n",
    "            drive_days,\n",
    "            failures\n",
    "        from drive_days\n",
    "        left join failures\n",
    "        on\n",
    "            drive_days.model = failures.model\"\"\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+\n",
      "|        model|drive_days|failures|\n",
      "+-------------+----------+--------+\n",
      "|  ST9250315AS|        89|    null|\n",
      "|  ST4000DM000|   1796728|      72|\n",
      "|ST12000NM0007|   3212635|     364|\n",
      "|  ST8000DM005|      2280|       1|\n",
      "|   ST320LT007|        89|    null|\n",
      "+-------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drive_days.join(failures, on=\"model\", how=\"left\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organizing SQL code via subqueries\n",
    "\n",
    "A subquery simply replaces a table name with a standalone SQL query. In the example, we can see that the name of the table has been replaced by the SELECT query that formed the table. We can alias the table referred to in the subquery by adding the name at the end of the statement, after the closing parenthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               model|        failure_rate|\n",
      "+--------------------+--------------------+\n",
      "|       ST12000NM0117|0.019305019305019305|\n",
      "|Seagate BarraCuda...|6.341154090044388E-4|\n",
      "|  TOSHIBA MQ01ABF050|5.579360828423496E-4|\n",
      "|         ST8000DM005|4.385964912280702E-4|\n",
      "|          ST500LM030| 4.19639110365086E-4|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT failures.model,\n",
    "       failures / drive_days failure_rate\n",
    "FROM   (\n",
    "        SELECT \n",
    "            model,\n",
    "            count(*) AS drive_days\n",
    "        FROM   drive_stats\n",
    "        GROUP  BY model) drive_days\n",
    "INNER JOIN (\n",
    "        SELECT \n",
    "            model,\n",
    "            count(*) AS failures\n",
    "        FROM   drive_stats\n",
    "        WHERE  failure = 1\n",
    "        GROUP  BY model) failures\n",
    "ON drive_days.model = failures.model\n",
    "ORDER  BY 2 DESC \n",
    "\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subqueries are cool but can be hard to read and debug, since you are adding complexity into the main query. This is where common table expressions, or CTEs, are especially useful. A CTE is a table definition, just like in the subquery case. The difference here is that you put them at the top of your main statement (before your main SELECT) and prefix with the word WITH. In the next listing, I take the same statement as the subquery case but use two CTE instead. These can also be considered makeshift CREATE statements that get dropped at the end of the query, just like the with keyword in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               model|        failure_rate|\n",
      "+--------------------+--------------------+\n",
      "|       ST12000NM0117|0.019305019305019305|\n",
      "|Seagate BarraCuda...|6.341154090044388E-4|\n",
      "|  TOSHIBA MQ01ABF050|5.579360828423496E-4|\n",
      "|         ST8000DM005|4.385964912280702E-4|\n",
      "|          ST500LM030| 4.19639110365086E-4|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "WITH drive_days as (\n",
    "    SELECT\n",
    "        model,\n",
    "        count(*) AS drive_days\n",
    "    FROM drive_stats\n",
    "    GROUP BY model),\n",
    "failures as (\n",
    "    SELECT\n",
    "        model,\n",
    "        count(*) AS failures\n",
    "    FROM drive_stats\n",
    "    WHERE failure = 1\n",
    "    GROUP BY model)\n",
    "\n",
    "SELECT\n",
    "    failures.model,\n",
    "    failures / drive_days failure_rate\n",
    "FROM drive_days\n",
    "INNER JOIN failures\n",
    "ON\n",
    "    drive_days.model = failures.model\n",
    "ORDER BY 2 desc\n",
    "\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+--------------------+\n",
      "|               model|drive_days|failures|        failure_rate|\n",
      "+--------------------+----------+--------+--------------------+\n",
      "|       ST12000NM0117|       259|       5|0.019305019305019305|\n",
      "|Seagate BarraCuda...|      1577|       1|6.341154090044388E-4|\n",
      "|  TOSHIBA MQ01ABF050|     44808|      25|5.579360828423496E-4|\n",
      "|         ST8000DM005|      2280|       1|4.385964912280702E-4|\n",
      "|          ST500LM030|     21447|       9| 4.19639110365086E-4|\n",
      "+--------------------+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def failure_rate(drive_stats):\n",
    "    \n",
    "    drive_days = drive_stats.groupby(F.col(\"model\")).agg(\n",
    "        F.count(F.col(\"*\")).alias(\"drive_days\")\n",
    "    )\n",
    "\n",
    "    failures = (\n",
    "        drive_stats.where(F.col(\"failure\") == 1)\n",
    "        .groupby(F.col(\"model\"))\n",
    "        .agg(F.count(F.col(\"*\")).alias(\"failures\"))\n",
    "    )\n",
    "    answer = (\n",
    "        drive_days.join(failures, on=\"model\", how=\"inner\")\n",
    "        .withColumn(\"failure_rate\", F.col(\"failures\") / F.col(\"drive_days\"))\n",
    "        .orderBy(F.col(\"failure_rate\").desc())\n",
    "        )\n",
    "\n",
    "    return answer \n",
    "\n",
    "failure_rate(backblaze_2019).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Python to increase resiliency and simplifying the data reading stage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05de79a9bc4beb95fb2b07d395d8e3fe55e6d8497bda19361fbfb16b724883dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
