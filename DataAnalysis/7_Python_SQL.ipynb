{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bilingual PySpark: Blending Python & SQL code\n",
    "\n",
    "In this section, we'll see how we can use python and SQL together with PySpark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 15:30:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Let's import from libs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing  pyspark.sql vs. plain SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|period|count|\n",
      "+------+-----+\n",
      "|     6|    1|\n",
      "|     4|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elements = spark.read.csv(\n",
    "    \"data/elements/Periodic_Table_Of_Elements.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "elements.where(F.col(\"phase\") == \"liq\").groupby(\"period\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT\n",
    "    period,\n",
    "    count(*)\n",
    "FROM \n",
    "    elements\n",
    "WHERE \n",
    "    phase = 'liq'\n",
    "GROUP BY \n",
    "    period;\n",
    "```\n",
    "\n",
    "<img src=\"images/python_sql_comparison.png\" height=\"300px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how to get spark data frame using SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table or view not found: elements; line 1 pos 29;\n",
      "'Aggregate ['period], ['period, unresolvedalias(count(1), None)]\n",
      "+- 'Filter ('phase = liq)\n",
      "   +- 'UnresolvedRelation [elements], [], false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql(\n",
    "        \"select period, count(*) from elements \"\n",
    "        \"where phase='liq' group by period\"\n",
    "    ).show(5)\n",
    "except AnalysisException as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, PySpark doesn’t make the link between the python variable elements, which points to the data frame, and a potential table elements that can be queried by Spark SQL. To allow a data frame to be queried via SQL, we need to **register** it.\n",
    "\n",
    "When you want to create a table/view to query with Spark SQL, use the createOrReplaceTempView() method. This method takes a single string parameter, which is the name of the table you want to use. This transformation will look at the data frame referenced by the Python variable on which the method was applied and will create a Spark SQL reference to the same data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 15:30:47 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+------+--------+\n",
      "|period|count(1)|\n",
      "+------+--------+\n",
      "|     6|       1|\n",
      "|     4|       1|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elements.createOrReplaceTempView(\"elements\")\n",
    "\n",
    "spark.sql(\n",
    "    \"select period, count(*) from elements where phase='liq' group by period\"\n",
    ").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how to manage these registered views/tables. Spark has a way of managing it via the **Catalog**. PySpark has four methods to create temporary views, and they look quite similar at first glance:\n",
    "- `createGlobalTempView()`\n",
    "- `createOrReplaceGlobalTempView()`\n",
    "- `createOrReplaceTempView()`\n",
    "- `createTempView()`\n",
    "\n",
    "The Spark catalog is an object that allows working with Spark SQL tables and views. A lot of its methods have to do with managing the metadata of those tables, such as their names and the level of caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.catalog.Catalog at 0x7fc3b8279f70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='elements', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropTempView(\"elements\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL and PySpark\n",
    "\n",
    "data downloaded from [here](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q3_2019.zip). To know more about the dataset check this [link](http://mng.bz/4jZa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "DIRECTORY = \"data/backblaze/\"\n",
    "\n",
    "backblaze_2019 = spark.read.csv(\n",
    "    DIRECTORY+\"*.csv\", \n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the layout for each column according to the schema\n",
    "backblaze_2019 = backblaze_2019.select(\n",
    "    [\n",
    "        F.col(x).cast(T.LongType()) if x.startswith(\"smart\") else F.col(x)\n",
    "        for x in backblaze_2019.columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "backblaze_2019.createOrReplaceTempView(\"backblaze_stats_2019\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: to perform a quick exploratory data analysis on a subset of the columns presented. Then will reproduce the failure rates that Backblaze computes and identify the models with the greatest and least amount of failures in 2019."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the rows and columns you want: select and where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|serial_number|\n",
      "+-------------+\n",
      "|     ZA10MCJ5|\n",
      "|     ZCH07T9K|\n",
      "|     ZCH0CA7Z|\n",
      "|     Z302F381|\n",
      "|     ZCH0B3Z2|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"select serial_number from backblaze_stats_2019 where failure = 1\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|serial_number|\n",
      "+-------------+\n",
      "|     ZA10MCJ5|\n",
      "|     ZCH07T9K|\n",
      "|     ZCH0CA7Z|\n",
      "|     Z302F381|\n",
      "|     ZCH0B3Z2|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "backblaze_2019.where(\"failure = 1\").select(F.col(\"serial_number\")).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping similar records together: group by and order by\n",
    "\n",
    "Let's start looking at the capacity, in gigabytes, of the hard drives included in the data, by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:==================================================>     (28 + 3) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               model|              min_GB| max_GB|\n",
      "+--------------------+--------------------+-------+\n",
      "| TOSHIBA MG07ACA14TA|             13039.0|13039.0|\n",
      "|       ST12000NM0117|             11176.0|11176.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALE600|             11176.0|11176.0|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"SELECT\n",
    "        model,\n",
    "        min(capacity_bytes / pow(1024, 3)) min_GB,\n",
    "        max(capacity_bytes/ pow(1024, 3)) max_GB\n",
    "    FROM backblaze_stats_2019\n",
    "    GROUP BY 1\n",
    "    ORDER BY 3 DESC\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:==================================================>     (28 + 3) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               model|              min_GB| max_GB|\n",
      "+--------------------+--------------------+-------+\n",
      "| TOSHIBA MG07ACA14TA|             13039.0|13039.0|\n",
      "|       ST12000NM0117|             11176.0|11176.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALE600|             11176.0|11176.0|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "backblaze_2019.groupby(F.col(\"model\")).agg(\n",
    "    F.min(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"min_GB\"),\n",
    "    F.max(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"max_GB\"),\n",
    ").orderBy(\n",
    "    F.col(\"max_GB\"), ascending=False\n",
    "    ).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering after grouping using having\n",
    "\n",
    "Looking at the results from our query, there are some drives that report more than one capacity. Furthermore, we have some drives that report negative capacity, which is really odd. Let’s focus on seeing how prevalent this is.\n",
    "\n",
    "Because of the order of the evaluation of operations in SQL, where is always applied before group by. What happens if we want to filter the values of columns created after the group by operation? We use a new keyword: `having`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=============================================>          (25 + 6) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+\n",
      "|               model|              min_GB|           max_GB|\n",
      "+--------------------+--------------------+-----------------+\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|          11176.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|          11176.0|\n",
      "|HGST HUH721010ALE600|-9.31322574615478...|           9314.0|\n",
      "|       ST10000NM0086|-9.31322574615478...|           9314.0|\n",
      "|        ST8000NM0055|-9.31322574615478...|7452.036460876465|\n",
      "+--------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"SELECT\n",
    "        model,\n",
    "        min(capacity_bytes / pow(1024, 3)) min_GB,\n",
    "        max(capacity_bytes/ pow(1024, 3)) max_GB\n",
    "    FROM backblaze_stats_2019\n",
    "    GROUP BY 1\n",
    "    HAVING min_GB != max_GB\n",
    "    ORDER BY 3 DESC\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:==============================================>         (26 + 5) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+\n",
      "|               model|              min_GB|           max_GB|\n",
      "+--------------------+--------------------+-----------------+\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|          11176.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|          11176.0|\n",
      "|HGST HUH721010ALE600|-9.31322574615478...|           9314.0|\n",
      "|       ST10000NM0086|-9.31322574615478...|           9314.0|\n",
      "|        ST8000NM0055|-9.31322574615478...|7452.036460876465|\n",
      "+--------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "backblaze_2019.groupby(F.col(\"model\")).agg(\n",
    "    F.min(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"min_GB\"),\n",
    "    F.max(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"max_GB\"),\n",
    ").where(F.col(\"min_GB\") != F.col(\"max_GB\")).orderBy(\n",
    "    F.col(\"max_GB\"), ascending=False\n",
    ").show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s materialize our work, SQL-style\n",
    "#### Creating new tables/views using the CREATE keyword\n",
    "\n",
    "Creating a table or a view is very easy in SQL: prefix our query by CREATE TABLE/VIEW. \n",
    "Let's reproduce the drive_days and failures that compute the number of days of operation that a model has and the number of drive failures it has had, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "backblaze_2019.createOrReplaceTempView(\"drive_stats\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW drive_days AS\n",
    "        SELECT model, count(*) AS drive_days\n",
    "        FROM drive_stats\n",
    "        GROUP BY model\"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"CREATE OR REPLACE TEMP VIEW failures AS\n",
    "        SELECT model, count(*) AS failures\n",
    "        FROM drive_stats\n",
    "        WHERE failure = 1\n",
    "        GROUP BY model\"\"\"\n",
    ")\n",
    "\n",
    "drive_days = backblaze_2019.groupby(F.col(\"model\")).agg(\n",
    "    F.count(F.col(\"*\")).alias(\"drive_days\")\n",
    ")\n",
    "\n",
    "failures = (\n",
    "    backblaze_2019.where(F.col(\"failure\") == 1)\n",
    "    .groupby(F.col(\"model\"))\n",
    "    .agg(F.count(F.col(\"*\")).alias(\"failures\"))\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: ***Creating tables from data in SQL***: You can also create a table from data on a hard drive or HDFS. For this, you can use a modified SQL query. Since we are reading a CSV file, we prefix our path with csv.: ``spark.sql(\"create table q1 as select * from csv.`./data/backblaze/drive_stats_2019_Q1`\")``"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding data to our table using UNION and JOIN\n",
    "\n",
    "Joins and unions are the only clauses we’ll see that modify the target piece in our SQL statement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "columns_backblaze = \", \".join(q4.columns)\n",
    "\n",
    "q1.createOrReplaceTempView(\"Q1\")\n",
    "q2.createOrReplaceTempView(\"Q2\")\n",
    "q3.createOrReplaceTempView(\"Q3\")\n",
    "q4.createOrReplaceTempView(\"Q4\")\n",
    "\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW backblaze_2019 AS\n",
    "    SELECT {col} FROM Q1 UNION ALL\n",
    "    SELECT {col} FROM Q2 UNION ALL\n",
    "    SELECT {col} FROM Q3 UNION ALL\n",
    "    SELECT {col} FROM Q4\n",
    "\"\"\".format(\n",
    "    col=columns_backblaze\n",
    "    )\n",
    ")\n",
    "\n",
    "backblaze_2019 = (\n",
    "    q1.select(q4.columns)\n",
    "    .union(q2.select(q4.columns))\n",
    "    .union(q3.select(q4.columns))\n",
    "    .union(q4)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+\n",
      "|        model|drive_days|failures|\n",
      "+-------------+----------+--------+\n",
      "|  ST9250315AS|        89|    null|\n",
      "|  ST4000DM000|   1796728|      72|\n",
      "|ST12000NM0007|   3212635|     364|\n",
      "|  ST8000DM005|      2280|       1|\n",
      "|   ST320LT007|        89|    null|\n",
      "+-------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining our tables\n",
    "spark.sql(\n",
    "    \"\"\"select\n",
    "            drive_days.model,\n",
    "            drive_days,\n",
    "            failures\n",
    "        from drive_days\n",
    "        left join failures\n",
    "        on\n",
    "            drive_days.model = failures.model\"\"\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:================================================>       (27 + 4) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+\n",
      "|        model|drive_days|failures|\n",
      "+-------------+----------+--------+\n",
      "|  ST9250315AS|        89|    null|\n",
      "|  ST4000DM000|   1796728|      72|\n",
      "|ST12000NM0007|   3212635|     364|\n",
      "|  ST8000DM005|      2280|       1|\n",
      "|   ST320LT007|        89|    null|\n",
      "+-------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "drive_days.join(failures, on=\"model\", how=\"left\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organizing SQL code via subqueries\n",
    "\n",
    "A subquery simply replaces a table name with a standalone SQL query. In the example, we can see that the name of the table has been replaced by the SELECT query that formed the table. We can alias the table referred to in the subquery by adding the name at the end of the statement, after the closing parenthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:======================================================> (30 + 1) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               model|        failure_rate|\n",
      "+--------------------+--------------------+\n",
      "|       ST12000NM0117|0.019305019305019305|\n",
      "|Seagate BarraCuda...|6.341154090044388E-4|\n",
      "|  TOSHIBA MQ01ABF050|5.579360828423496E-4|\n",
      "|         ST8000DM005|4.385964912280702E-4|\n",
      "|          ST500LM030| 4.19639110365086E-4|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT failures.model,\n",
    "       failures / drive_days failure_rate\n",
    "FROM   (\n",
    "        SELECT \n",
    "            model,\n",
    "            count(*) AS drive_days\n",
    "        FROM   drive_stats\n",
    "        GROUP  BY model) drive_days\n",
    "INNER JOIN (\n",
    "        SELECT \n",
    "            model,\n",
    "            count(*) AS failures\n",
    "        FROM   drive_stats\n",
    "        WHERE  failure = 1\n",
    "        GROUP  BY model) failures\n",
    "ON drive_days.model = failures.model\n",
    "ORDER  BY 2 DESC \n",
    "\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subqueries are cool but can be hard to read and debug, since you are adding complexity into the main query. This is where common table expressions, or CTEs, are especially useful. A CTE is a table definition, just like in the subquery case. The difference here is that you put them at the top of your main statement (before your main SELECT) and prefix with the word WITH. In the next listing, I take the same statement as the subquery case but use two CTE instead. These can also be considered makeshift CREATE statements that get dropped at the end of the query, just like the with keyword in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:====================================================>   (29 + 2) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               model|        failure_rate|\n",
      "+--------------------+--------------------+\n",
      "|       ST12000NM0117|0.019305019305019305|\n",
      "|Seagate BarraCuda...|6.341154090044388E-4|\n",
      "|  TOSHIBA MQ01ABF050|5.579360828423496E-4|\n",
      "|         ST8000DM005|4.385964912280702E-4|\n",
      "|          ST500LM030| 4.19639110365086E-4|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "WITH drive_days as (\n",
    "    SELECT\n",
    "        model,\n",
    "        count(*) AS drive_days\n",
    "    FROM drive_stats\n",
    "    GROUP BY model),\n",
    "failures as (\n",
    "    SELECT\n",
    "        model,\n",
    "        count(*) AS failures\n",
    "    FROM drive_stats\n",
    "    WHERE failure = 1\n",
    "    GROUP BY model)\n",
    "\n",
    "SELECT\n",
    "    failures.model,\n",
    "    failures / drive_days failure_rate\n",
    "FROM drive_days\n",
    "INNER JOIN failures\n",
    "ON\n",
    "    drive_days.model = failures.model\n",
    "ORDER BY 2 desc\n",
    "\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:====================================================>   (29 + 2) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+--------------------+\n",
      "|               model|drive_days|failures|        failure_rate|\n",
      "+--------------------+----------+--------+--------------------+\n",
      "|       ST12000NM0117|       259|       5|0.019305019305019305|\n",
      "|Seagate BarraCuda...|      1577|       1|6.341154090044388E-4|\n",
      "|  TOSHIBA MQ01ABF050|     44808|      25|5.579360828423496E-4|\n",
      "|         ST8000DM005|      2280|       1|4.385964912280702E-4|\n",
      "|          ST500LM030|     21447|       9| 4.19639110365086E-4|\n",
      "+--------------------+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def failure_rate(drive_stats):\n",
    "    \n",
    "    drive_days = drive_stats.groupby(F.col(\"model\")).agg(\n",
    "        F.count(F.col(\"*\")).alias(\"drive_days\")\n",
    "    )\n",
    "\n",
    "    failures = (\n",
    "        drive_stats.where(F.col(\"failure\") == 1)\n",
    "        .groupby(F.col(\"model\"))\n",
    "        .agg(F.count(F.col(\"*\")).alias(\"failures\"))\n",
    "    )\n",
    "    answer = (\n",
    "        drive_days.join(failures, on=\"model\", how=\"inner\")\n",
    "        .withColumn(\"failure_rate\", F.col(\"failures\") / F.col(\"drive_days\"))\n",
    "        .orderBy(F.col(\"failure_rate\").desc())\n",
    "        )\n",
    "\n",
    "    return answer \n",
    "\n",
    "failure_rate(backblaze_2019).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Python to increase resiliency and simplifying the data reading stage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "DATA_DIRECTORY = \"data/backblaze/\"\n",
    "DATA_FILES = ['2019-09-*.csv']\n",
    "\n",
    "data = [\n",
    "    spark.read.csv(DATA_DIRECTORY + file, header=True, inferSchema=True)\n",
    "    for file in DATA_FILES\n",
    "]\n",
    "\n",
    "common_columns = list(\n",
    "    reduce(lambda x, y: x.intersection(y), [set(df.columns) for df in data])\n",
    ")\n",
    "\n",
    "assert set([\"model\", \"capacity_bytes\", \"date\", \"failure\"]).issubset(\n",
    "    set(common_columns)\n",
    ")\n",
    "\n",
    "full_data = reduce(\n",
    "    lambda x, y: x.select(common_columns).union(y.select(common_columns)), data\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SQL-style expressions in PySpark\n",
    "\n",
    "In this section, we use SQL-style expressions when appropriate to showcase when it makes sense to fuse both languages. At the end of this block, we have code that\n",
    "- Selects only the useful columns for our query\n",
    "- Gets our drive capacity in gigabytes\n",
    "- Computes the drive_days and failures data frames\n",
    "- Joins the two data frames into a summarized one and computes the failure rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = full_data.selectExpr(\n",
    "    \"model\", \"capacity_bytes / pow(1024, 3) capacity_GB\", \"date\", \"failure\"\n",
    ")\n",
    "\n",
    "drive_days = full_data.groupby(\"model\", \"capacity_GB\").agg(\n",
    "    F.count(\"*\").alias(\"drive_days\")\n",
    ")\n",
    "\n",
    "failures = (\n",
    "    full_data.where(\"failure = 1\")\n",
    "    .groupby(\"model\", \"capacity_GB\")\n",
    "    .agg(F.count(\"*\").alias(\"failures\"))\n",
    ")\n",
    "\n",
    "summarized_data = (\n",
    "    drive_days.join(failures, on=[\"model\", \"capacity_GB\"], how=\"left\")\n",
    "    .fillna(0.0, [\"failures\"])\n",
    "    .selectExpr(\"model\", \"capacity_GB\", \"failures / drive_days failure_rate\")\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`selectExpr()` is just like the `select()` method with the exception that it will process SQL-style operations. This is nice since it removes a bit of syntax when manipulating columns with functions and arithmetic. In our case, the PySpark alternative (displayed in the next listing) is a little more verbose and cumbersome to write and read, especially since we have to create a literal `1024` column to apply the `pow()` function.\n",
    "\n",
    "```python\n",
    "full_data = full_data.select(\n",
    "    F.col(\"model\"),\n",
    "    (F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"capacity_GB\"),\n",
    "    F.col(\"date\"),\n",
    "    F.col(\"failure\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method is simply called `expr()`. It wraps a SQL-style expression into a column. This is kind of a generalized `selectExpr()` that you can use in lieu of `F.col()` (or the column name) when you want to modify a column\n",
    "```python\n",
    "failures = (\n",
    "    full_data.where(\"failure = 1\")\n",
    "    .groupby(\"model\", \"capacity_GB\")\n",
    "    .agg(F.expr(\"count(*) failures\"))\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third method is the `where()/filter()` method. I find the syntax for filtering in SQL much less verbose than regular PySpark; being able to use the SQL syntax as the argument of the filter() method with no ceremony is a godsend. In our final program, I am able to use `full_data.where(\"failure = 1\")` instead of wrapping the column name in `F.col()` like we’ve been doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 77:=======================================>              (148 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+\n",
      "|               model|capacity_GB|        failure_rate|\n",
      "+--------------------+-----------+--------------------+\n",
      "|HGST HUH721010ALE600|     9314.0|                 0.0|\n",
      "|HGST HUH721212ALE600|    11176.0|2.136752136752136...|\n",
      "|HGST HUH721212ALN604|    11176.0|2.150346052118244...|\n",
      "+--------------------+-----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def most_reliable_drive_for_capacity(data, capacity_GB=2048, precision=0.25, top_n=3):\n",
    "    \"\"\"Returns the top 3 drives for a given approximate capacity.\n",
    "    Given a capacity in GB and a precision as a decimal number, we keep the N\n",
    "    drives where:\n",
    "    - the capacity is between (capacity * 1/(1+precision)), capacity *\n",
    "    (1+precision)\n",
    "    - the failure rate is the lowest\n",
    "    \"\"\"\n",
    "    capacity_min = capacity_GB / (1 + precision)\n",
    "    capacity_max = capacity_GB * (1 + precision)\n",
    "    answer = (\n",
    "        data.where(f\"capacity_GB between {capacity_min} and {capacity_max}\")\n",
    "        .orderBy(\"failure_rate\", \"capacity_GB\", ascending=[True, False])\n",
    "        .limit(top_n)\n",
    "    )\n",
    "    return answer\n",
    "most_reliable_drive_for_capacity(summarized_data, capacity_GB=11176.0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b877a15d2533b687b493c627c629234c94445540cc0e731a6ce010c066d5499"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
