{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bilingual PySpark: Blending Python & SQL code\n",
    "\n",
    "In this section, we'll see how we can use python and SQL together with PySpark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import from libs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing  pyspark.sql vs. plain SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|period|count|\n",
      "+------+-----+\n",
      "|     6|    1|\n",
      "|     4|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elements = spark.read.csv(\n",
    "    \"data/elements/Periodic_Table_Of_Elements.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "elements.where(F.col(\"phase\") == \"liq\").groupby(\"period\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT\n",
    "    period,\n",
    "    count(*)\n",
    "FROM \n",
    "    elements\n",
    "WHERE \n",
    "    phase = 'liq'\n",
    "GROUP BY \n",
    "    period;\n",
    "```\n",
    "\n",
    "<img src=\"images/python_sql_comparison.png\" height=\"300px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how to get spark data frame using SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table or view not found: elements; line 1 pos 29;\n",
      "'Aggregate ['period], ['period, unresolvedalias(count(1), None)]\n",
      "+- 'Filter ('phase = liq)\n",
      "   +- 'UnresolvedRelation [elements], [], false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql(\n",
    "        \"select period, count(*) from elements \"\n",
    "        \"where phase='liq' group by period\"\n",
    "    ).show(5)\n",
    "except AnalysisException as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, PySpark doesn’t make the link between the python variable elements, which points to the data frame, and a potential table elements that can be queried by Spark SQL. To allow a data frame to be queried via SQL, we need to **register** it.\n",
    "\n",
    "When you want to create a table/view to query with Spark SQL, use the createOrReplaceTempView() method. This method takes a single string parameter, which is the name of the table you want to use. This transformation will look at the data frame referenced by the Python variable on which the method was applied and will create a Spark SQL reference to the same data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|period|count(1)|\n",
      "+------+--------+\n",
      "|     6|       1|\n",
      "|     4|       1|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elements.createOrReplaceTempView(\"elements\")\n",
    "\n",
    "spark.sql(\n",
    "    \"select period, count(*) from elements where phase='liq' group by period\"\n",
    ").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how to manage these registered views/tables. Spark has a way of managing it via the **Catalog**. PySpark has four methods to create temporary views, and they look quite similar at first glance:\n",
    "- `createGlobalTempView()`\n",
    "- `createOrReplaceGlobalTempView()`\n",
    "- `createOrReplaceTempView()`\n",
    "- `createTempView()`\n",
    "\n",
    "The Spark catalog is an object that allows working with Spark SQL tables and views. A lot of its methods have to do with managing the metadata of those tables, such as their names and the level of caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.catalog.Catalog at 0x2761c3812b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='backblaze_stats_2019', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='drive_days', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='drive_stats', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='elements', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='failures', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='backblaze_stats_2019', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='drive_days', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='drive_stats', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='failures', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropTempView(\"elements\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL and PySpark\n",
    "\n",
    "data downloaded from [here](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q3_2019.zip). To know more about the dataset check this [link](http://mng.bz/4jZa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"data/backblaze/\"\n",
    "\n",
    "backblaze_2019 = spark.read.csv(\n",
    "    DIRECTORY+\"*.csv\", \n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the layout for each column according to the schema\n",
    "backblaze_2019 = backblaze_2019.select(\n",
    "    [\n",
    "        F.col(x).cast(T.LongType()) if x.startswith(\"smart\") else F.col(x)\n",
    "        for x in backblaze_2019.columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "backblaze_2019.createOrReplaceTempView(\"backblaze_stats_2019\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: to perform a quick exploratory data analysis on a subset of the columns presented. Then will reproduce the failure rates that Backblaze computes and identify the models with the greatest and least amount of failures in 2019."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the rows and columns you want: select and where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|serial_number|\n",
      "+-------------+\n",
      "|     ZA10MCJ5|\n",
      "|     ZCH07T9K|\n",
      "|     ZCH0CA7Z|\n",
      "|     Z302F381|\n",
      "|     ZCH0B3Z2|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"select serial_number from backblaze_stats_2019 where failure = 1\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|serial_number|\n",
      "+-------------+\n",
      "|     ZA10MCJ5|\n",
      "|     ZCH07T9K|\n",
      "|     ZCH0CA7Z|\n",
      "|     Z302F381|\n",
      "|     ZCH0B3Z2|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "backblaze_2019.where(\"failure = 1\").select(F.col(\"serial_number\")).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping similar records together: group by and order by\n",
    "\n",
    "Let's start looking at the capacity, in gigabytes, of the hard drives included in the data, by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               model|              min_GB| max_GB|\n",
      "+--------------------+--------------------+-------+\n",
      "| TOSHIBA MG07ACA14TA|             13039.0|13039.0|\n",
      "|       ST12000NM0117|             11176.0|11176.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALE600|             11176.0|11176.0|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"SELECT\n",
    "        model,\n",
    "        min(capacity_bytes / pow(1024, 3)) min_GB,\n",
    "        max(capacity_bytes/ pow(1024, 3)) max_GB\n",
    "    FROM backblaze_stats_2019\n",
    "    GROUP BY 1\n",
    "    ORDER BY 3 DESC\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               model|              min_GB| max_GB|\n",
      "+--------------------+--------------------+-------+\n",
      "| TOSHIBA MG07ACA14TA|             13039.0|13039.0|\n",
      "|       ST12000NM0117|             11176.0|11176.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALE600|             11176.0|11176.0|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "backblaze_2019.groupby(F.col(\"model\")).agg(\n",
    "    F.min(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"min_GB\"),\n",
    "    F.max(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"max_GB\"),\n",
    ").orderBy(\n",
    "    F.col(\"max_GB\"), ascending=False\n",
    "    ).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering after grouping using having\n",
    "\n",
    "Looking at the results from our query, there are some drives that report more than one capacity. Furthermore, we have some drives that report negative capacity, which is really odd. Let’s focus on seeing how prevalent this is.\n",
    "\n",
    "Because of the order of the evaluation of operations in SQL, where is always applied before group by. What happens if we want to filter the values of columns created after the group by operation? We use a new keyword: `having`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+\n",
      "|               model|              min_GB|           max_GB|\n",
      "+--------------------+--------------------+-----------------+\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|          11176.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|          11176.0|\n",
      "|HGST HUH721010ALE600|-9.31322574615478...|           9314.0|\n",
      "|       ST10000NM0086|-9.31322574615478...|           9314.0|\n",
      "|        ST8000NM0055|-9.31322574615478...|7452.036460876465|\n",
      "+--------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"SELECT\n",
    "        model,\n",
    "        min(capacity_bytes / pow(1024, 3)) min_GB,\n",
    "        max(capacity_bytes/ pow(1024, 3)) max_GB\n",
    "    FROM backblaze_stats_2019\n",
    "    GROUP BY 1\n",
    "    HAVING min_GB != max_GB\n",
    "    ORDER BY 3 DESC\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+\n",
      "|               model|              min_GB|           max_GB|\n",
      "+--------------------+--------------------+-----------------+\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|          11176.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|          11176.0|\n",
      "|HGST HUH721010ALE600|-9.31322574615478...|           9314.0|\n",
      "|       ST10000NM0086|-9.31322574615478...|           9314.0|\n",
      "|        ST8000NM0055|-9.31322574615478...|7452.036460876465|\n",
      "+--------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "backblaze_2019.groupby(F.col(\"model\")).agg(\n",
    "    F.min(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"min_GB\"),\n",
    "    F.max(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"max_GB\"),\n",
    ").where(F.col(\"min_GB\") != F.col(\"max_GB\")).orderBy(\n",
    "    F.col(\"max_GB\"), ascending=False\n",
    ").show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s materialize our work, SQL-style\n",
    "#### Creating new tables/views using the CREATE keyword\n",
    "\n",
    "Creating a table or a view is very easy in SQL: prefix our query by CREATE TABLE/VIEW. \n",
    "Let's reproduce the drive_days and failures that compute the number of days of operation that a model has and the number of drive failures it has had, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "backblaze_2019.createOrReplaceTempView(\"drive_stats\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW drive_days AS\n",
    "        SELECT model, count(*) AS drive_days\n",
    "        FROM drive_stats\n",
    "        GROUP BY model\"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"CREATE OR REPLACE TEMP VIEW failures AS\n",
    "        SELECT model, count(*) AS failures\n",
    "        FROM drive_stats\n",
    "        WHERE failure = 1\n",
    "        GROUP BY model\"\"\"\n",
    ")\n",
    "\n",
    "drive_days = backblaze_2019.groupby(F.col(\"model\")).agg(\n",
    "    F.count(F.col(\"*\")).alias(\"drive_days\")\n",
    ")\n",
    "\n",
    "failures = (\n",
    "    backblaze_2019.where(F.col(\"failure\") == 1)\n",
    "    .groupby(F.col(\"model\"))\n",
    "    .agg(F.count(F.col(\"*\")).alias(\"failures\"))\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: ***Creating tables from data in SQL***: You can also create a table from data on a hard drive or HDFS. For this, you can use a modified SQL query. Since we are reading a CSV file, we prefix our path with csv.: ``spark.sql(\"create table q1 as select * from csv.`./data/backblaze/drive_stats_2019_Q1`\")``"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding data to our table using UNION and JOIN\n",
    "\n",
    "Joins and unions are the only clauses we’ll see that modify the target piece in our SQL statement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "columns_backblaze = \", \".join(q4.columns)\n",
    "\n",
    "q1.createOrReplaceTempView(\"Q1\")\n",
    "q2.createOrReplaceTempView(\"Q2\")\n",
    "q3.createOrReplaceTempView(\"Q3\")\n",
    "q4.createOrReplaceTempView(\"Q4\")\n",
    "\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW backblaze_2019 AS\n",
    "    SELECT {col} FROM Q1 UNION ALL\n",
    "    SELECT {col} FROM Q2 UNION ALL\n",
    "    SELECT {col} FROM Q3 UNION ALL\n",
    "    SELECT {col} FROM Q4\n",
    "\"\"\".format(\n",
    "    col=columns_backblaze\n",
    "    )\n",
    ")\n",
    "\n",
    "backblaze_2019 = (\n",
    "    q1.select(q4.columns)\n",
    "    .union(q2.select(q4.columns))\n",
    "    .union(q3.select(q4.columns))\n",
    "    .union(q4)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+\n",
      "|        model|drive_days|failures|\n",
      "+-------------+----------+--------+\n",
      "|  ST9250315AS|        89|    null|\n",
      "|  ST4000DM000|   1796728|      72|\n",
      "|ST12000NM0007|   3212635|     364|\n",
      "|  ST8000DM005|      2280|       1|\n",
      "|   ST320LT007|        89|    null|\n",
      "+-------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining our tables\n",
    "spark.sql(\n",
    "    \"\"\"select\n",
    "            drive_days.model,\n",
    "            drive_days,\n",
    "            failures\n",
    "        from drive_days\n",
    "        left join failures\n",
    "        on\n",
    "            drive_days.model = failures.model\"\"\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+\n",
      "|        model|drive_days|failures|\n",
      "+-------------+----------+--------+\n",
      "|  ST9250315AS|        89|    null|\n",
      "|  ST4000DM000|   1796728|      72|\n",
      "|ST12000NM0007|   3212635|     364|\n",
      "|  ST8000DM005|      2280|       1|\n",
      "|   ST320LT007|        89|    null|\n",
      "+-------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drive_days.join(failures, on=\"model\", how=\"left\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05de79a9bc4beb95fb2b07d395d8e3fe55e6d8497bda19361fbfb16b724883dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
