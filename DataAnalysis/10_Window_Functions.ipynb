{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your data under a different lens: Window functions\n",
    "\n",
    "On first glance, they look like a watered-down version of the split-applycombine pattern introduced in previous notebook [Pandas UDFs](./9_Pandas_UDF.ipynb). But it contains powerful manipulations in a short and expressive body of code.\n",
    "\n",
    "Window functions fill a niche between group aggregate (groupBy().agg()) and\n",
    "group map UDF (groupBy().apply()) transformations, both seen in previous notebook [Pandas UDFs](./9_Pandas_UDF.ipynb). Both rely on partitioning to split the data frame based on a predicate. A group aggregate transformation will yield one record per grouping, while a group map UDF allows for any shape of a resulting data frame; a window function always keeps the dimensions of the data frame intact. Window functions have a secret weapon in the window frame that we define within a partition: it determines which records are included in the application of the function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window functions are mostly used for creating new columns, so they leverage\n",
    "some familiar methods, such as select() and withColumn(). But we will see different approach here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growing and using a simle window function\n",
    "\n",
    "For this section, we reuse the temperature data set from [RDD & UDF](./8_RDD_n_UDFs.ipynb); the data set contains weather observations for a series of stations, summarized by day. Window functions especially shine when working with time series-like data (e.g., daily observations of temperature) because you can slice the data by day, month, or year and get useful statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "gsod = spark.read.parquet(\"./data/window/gsod.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying the coldest day of each year, the long way\n",
    "\n",
    "In this section, we emulate a simple window function through functionality we learned in previous notebooks using the `join()` method.\n",
    "\n",
    "we start with simple questions to ask our data frame: when and where were the lowest temperature recorded each year? In other words, we want a data frame containing three records, one for each year and showing the station, the date (year, month, day), and the temperature of the coldest day recorded for that year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|year|  temp|\n",
      "+----+------+\n",
      "|2019|-114.7|\n",
      "|2017|-114.7|\n",
      "|2018|-113.5|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coldest_temp = gsod.groupby(\"year\").agg(F.min(\"temp\").alias(\"temp\"))\n",
    "coldest_temp.orderBy(\"temp\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides the year and the temperature, which are about 40% of the original ask. To get the other three columns (`mo`, `da`, `stn`), we can use a left-semi join on the original table, using the results of `coldest_temp` to resolve the join.\n",
    "\n",
    "we join `gsod` to `coldest_temp` using a left-semi equi-join on the `year` and `temp` columns. Because `coldest_temp` only contains the coldest temperature for each year, the left semi-join keeps only the records from `gsod` that correspond to that year-temperature pair; this is equivalent to keeping only the records where the temperature was coldest for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+------+\n",
      "|   stn|year| mo| da|  temp|\n",
      "+------+----+---+---+------+\n",
      "|896250|2017| 06| 20|-114.7|\n",
      "|896060|2018| 08| 27|-113.5|\n",
      "|895770|2019| 06| 15|-114.7|\n",
      "+------+----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coldest_when = gsod.join(\n",
    "    coldest_temp, how=\"left_semi\", on=[\"year\", \"temp\"]\n",
    ").select(\"stn\", \"year\", \"mo\", \"da\", \"temp\")\n",
    "\n",
    "coldest_when.orderBy(\"year\", \"mo\", \"da\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above codes we are performing a join between the gsod table and, well, something coming from the gsod table. A self-join, which is when you join a table with itself, is often considered an anti-pattern for data manipulation. While it’s not technically wrong, it can be slow and make the code look more complex than it needs to be.\n",
    "\n",
    "<img src=\"images/self_join_table.png\">\n",
    "\n",
    "Fortunately, a window function gives you the same result faster, and with less code\n",
    "clutter. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and using a simple window function to get the coldest days\n",
    "\n",
    "We use the `Window` object and parameterize it to split a data frame over column values. We then apply the window over a data frame, using the traditional selector approach.\n",
    "\n",
    "Similar to split-apply-combine pattern we covered in previous notebooks, we will employ three stages\n",
    "- Instead of splitting, we’ll partition the data frame.\n",
    "- Instead of applying, we’ll select values over the window.\n",
    "- The combine/union operation is implicit (i.e., not explicitly coded) in a window function\n",
    "\n",
    "Window functions apply over a window of data split according to the values on a column. Each split, called a partition, gets the window function applied to each of its records as if they were independent data frames. The result then gets unioned back into a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.window.WindowSpec object at 0x000001F99B78B3A0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "each_year = Window.partitionBy(\"year\")\n",
    "\n",
    "print(each_year)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A WindowSpec object is nothing more than a blueprint for an eventual window function. We created a window specification called each_year that instructs the window application to split the data frame according to the values in the year column. The real magic happens when you apply the window function to your data\n",
    "frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+------+\n",
      "|   stn|year| mo| da|  temp|\n",
      "+------+----+---+---+------+\n",
      "|896250|2017| 06| 20|-114.7|\n",
      "|896060|2018| 08| 27|-113.5|\n",
      "|895770|2019| 06| 15|-114.7|\n",
      "+------+----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Self-Join Approach (repeat)\n",
    "\n",
    "coldest_when = gsod.join(\n",
    "    coldest_temp, how=\"left_semi\", on=[\"year\", \"temp\"]\n",
    ").select(\"stn\", \"year\", \"mo\", \"da\", \"temp\")\n",
    "\n",
    "coldest_when.orderBy(\"year\", \"mo\", \"da\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the `withColumn()` method we define a column, `min_temp`, that collects the minimum of the `temp` column. Now, rather than picking the minimum temperature of the whole data frame, the `min()` is applied over the window specification we defined, using the `over()` method. For each window partition, Spark computes the minimum and then broadcasts the value over each record.\n",
    "\n",
    "This is an important distinction compared to aggregating functions or UDF: in the\n",
    "case of a window function, *the number of records in the data frame does not change*. Although `min()` is an aggregate function, since it’s applied with the `over()` method, every record in the window has the minimum value appended. The same would apply for any other aggregate function from `pyspark.sql.functions`, such as `sum()`, `avg()`, `min()`, `max()`, and `count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+------+------+\n",
      "|year| mo| da|   stn|  temp|\n",
      "+----+---+---+------+------+\n",
      "|2017| 06| 20|896250|-114.7|\n",
      "|2018| 08| 27|896060|-113.5|\n",
      "|2019| 06| 15|895770|-114.7|\n",
      "+----+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window function\n",
    "\n",
    "(gsod\n",
    ".withColumn(\"min_temp\", F.min(\"temp\").over(each_year))\n",
    ".where(\"temp = min_temp\")\n",
    ".select(\"year\", \"mo\", \"da\", \"stn\", \"temp\")\n",
    ".orderBy(\"year\", \"mo\", \"da\")\n",
    ".show())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Window functions are just methods on columns**\\\n",
    "Since a window function is applied though a method on a `Column` object, you can also apply them in a `select()`. You can also apply more than one window (or different ones) within the same `select()`. Spark won’t allow you to use a window directly in a `groupby()` or `where()` method, where it’ll spit an `AnalysisException`. If you want to group by or filter according to the result of a window function, “materialize” the column using `select()` or `withColumn()` before using the desired operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+------+------+\n",
      "|year| mo| da|   stn|  temp|\n",
      "+----+---+---+------+------+\n",
      "|2017| 06| 20|896250|-114.7|\n",
      "|2018| 08| 27|896060|-113.5|\n",
      "|2019| 06| 15|895770|-114.7|\n",
      "+----+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod.select(\n",
    "    \"year\",\n",
    "    \"mo\",\n",
    "    \"da\",\n",
    "    \"stn\",\n",
    "    \"temp\",\n",
    "    F.min(\"temp\").over(each_year).alias(\"min_temp\"),\n",
    ").where(\n",
    "    \"temp = min_temp\"\n",
    ").drop(\n",
    "    \"min_temp\"\n",
    ").orderBy(\n",
    "    \"year\", \"mo\", \"da\"\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **But data frames already have partitions**\n",
    "Since the beginning of the notebooks, partition has referred to the physical splits of the data on each executor node. Now we are also using partitions with window functions to mean logical splits of the data, which may or may not be equal to the Spark physical ones.\n",
    "\n",
    "<img src=\"images/partitions.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond summarizing: Using ranking and analytical functions\n",
    "\n",
    "There are two families of functions which allow performance of a wider range of operations versus aggregate functions such as `count()`, `sum()`, or `min()`:\n",
    "\n",
    "- The `ranking` family, which provides information about rank (first, second, all\n",
    "the way to last), n-tiles, and the ever so useful row number.\n",
    "- The `analytical` family, which, despite its namesake, covers a variety of behaviors not related to summary or ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+\n",
      "|   stn|year| mo| da|temp|count_temp|\n",
      "+------+----+---+---+----+----------+\n",
      "|994979|2017| 12| 11|21.3|        21|\n",
      "|998012|2017| 03| 02|31.4|        24|\n",
      "|719200|2017| 10| 09|60.5|        11|\n",
      "|917350|2018| 04| 21|82.6|         9|\n",
      "|076470|2018| 06| 07|65.0|        24|\n",
      "|996470|2018| 03| 12|55.6|        12|\n",
      "|041680|2019| 02| 19|16.1|        15|\n",
      "|949110|2019| 11| 23|54.9|        14|\n",
      "|998252|2019| 04| 18|44.7|        11|\n",
      "|998166|2019| 03| 20|34.8|        12|\n",
      "+------+----+---+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod_light = spark.read.parquet(\"./data/window/gsod_light.parquet\")\n",
    "gsod_light.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking functions: Quick, who’s first?\n",
    "\n",
    "Ranking functions are used for getting the top (or bottom) record for each window partition, or, more generally, for getting an order according to some column’s value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_per_month_asc = Window.partitionBy(\"mo\").orderBy(\"count_temp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gold, Silver, Bronze: Simple Ranking using `rank()`\n",
    "\n",
    "With `rank()`, each record gets a position based on the value contained in one (or more) columns. Identical values have identical ranks—just like medalists in the Olympics, where the same score/time yields the same rank.\n",
    "\n",
    "`rank()` takes no parameters since it ranks according to the `orderBy()` method\n",
    "from the window spec; it would not make sense to order according to one column but\n",
    "rank according to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+--------+\n",
      "|   stn|year| mo| da|temp|count_temp|rank_tpm|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|041680|2019| 02| 19|16.1|        15|       1|\n",
      "|996470|2018| 03| 12|55.6|        12|       1|\n",
      "|998166|2019| 03| 20|34.8|        12|       1|\n",
      "|998012|2017| 03| 02|31.4|        24|       3|\n",
      "|917350|2018| 04| 21|82.6|         9|       1|\n",
      "|998252|2019| 04| 18|44.7|        11|       2|\n",
      "|076470|2018| 06| 07|65.0|        24|       1|\n",
      "|719200|2017| 10| 09|60.5|        11|       1|\n",
      "|949110|2019| 11| 23|54.9|        14|       1|\n",
      "|994979|2017| 12| 11|21.3|        21|       1|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod_light.withColumn(\n",
    "    \"rank_tpm\", F.rank().over(temp_per_month_asc)\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `rank()` provides nonconsecutive ranks for each record, based on the\n",
    "value of the ordered value, or the column(s) provided in the `orderBy()` method of\n",
    "the window spec we call. \n",
    "\n",
    "for each window, the lower the count_temp, the lower the rank. When two records have the same ordered value, their ranks are the same. We say that the rank is nonconsecutive because, when you have multiple records that tie for a rank, the next one will be offset by the number of ties. For instance, for `mo` = 03, we have two records with `count_temp` = 12: both are rank 1. The next record (`count_temp` = 24) has a position of 3 rather than 2, because two records tied for the first position."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Ties When Ranking: Using `dense_rank()`\n",
    "\n",
    "What if we want, say, a denser ranking that would allocate consecutive ranks for records? Enter `dense_rank()`. The same principle as rank() applies, where ties share the same rank, but there won’t be any gap between the ranks: 1, 2, 3, and so on. This is practical when you want the second (or third, or any ordinal position) value over a window, rather than the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+--------+\n",
      "|   stn|year| mo| da|temp|count_temp|rank_tpm|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|041680|2019| 02| 19|16.1|        15|       1|\n",
      "|996470|2018| 03| 12|55.6|        12|       1|\n",
      "|998166|2019| 03| 20|34.8|        12|       1|\n",
      "|998012|2017| 03| 02|31.4|        24|       2|\n",
      "|917350|2018| 04| 21|82.6|         9|       1|\n",
      "|998252|2019| 04| 18|44.7|        11|       2|\n",
      "|076470|2018| 06| 07|65.0|        24|       1|\n",
      "|719200|2017| 10| 09|60.5|        11|       1|\n",
      "|949110|2019| 11| 23|54.9|        14|       1|\n",
      "|994979|2017| 12| 11|21.3|        21|       1|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod_light.withColumn(\n",
    "    \"rank_tpm\", F.dense_rank().over(temp_per_month_asc)\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking? Scoring? `percent_rank()` gives you both!\n",
    "\n",
    "What if you want something closer to a scope, perhaps even a percentage that would\n",
    "reflect where a record stands compared to its peers in the same window partition?\n",
    "Enter `percent_rank()`.\n",
    "\n",
    "For every window, `percent_rank()` will compute the percentage rank (between\n",
    "zero and one) based on the ordered value. \n",
    "\n",
    "$$\\frac{\\# \\text{records with a lower value than the current one}}{\\# \\text{of records in the window} - 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+------------------+\n",
      "|   stn|year| mo| da|temp|count_temp|          rank_tpm|\n",
      "+------+----+---+---+----+----------+------------------+\n",
      "|994979|2017| 12| 11|21.3|        21|               0.0|\n",
      "|998012|2017| 03| 02|31.4|        24|               0.5|\n",
      "|719200|2017| 10| 09|60.5|        11|               1.0|\n",
      "|996470|2018| 03| 12|55.6|        12|               0.0|\n",
      "|076470|2018| 06| 07|65.0|        24|               0.5|\n",
      "|917350|2018| 04| 21|82.6|         9|               1.0|\n",
      "|041680|2019| 02| 19|16.1|        15|               0.0|\n",
      "|998166|2019| 03| 20|34.8|        12|0.3333333333333333|\n",
      "|998252|2019| 04| 18|44.7|        11|0.6666666666666666|\n",
      "|949110|2019| 11| 23|54.9|        14|               1.0|\n",
      "+------+----+---+---+----+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_each_year = each_year.orderBy(\"temp\") # create a window spec from another window spec by chaining\n",
    "\n",
    "gsod_light.withColumn(\n",
    "    \"rank_tpm\", F.percent_rank().over(temp_each_year)\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Buckets Based on Ranks, using `ntile()`\n",
    "\n",
    "`ntile()` functions allows you to create an arbitrary number of buckets (called tiles) based on the rank of your data. It computes n-tile for a given parameter `n`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+--------+\n",
      "|   stn|year| mo| da|temp|count_temp|rank_tpm|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|994979|2017| 12| 11|21.3|        21|       1|\n",
      "|998012|2017| 03| 02|31.4|        24|       1|\n",
      "|719200|2017| 10| 09|60.5|        11|       2|\n",
      "|996470|2018| 03| 12|55.6|        12|       1|\n",
      "|076470|2018| 06| 07|65.0|        24|       1|\n",
      "|917350|2018| 04| 21|82.6|         9|       2|\n",
      "|041680|2019| 02| 19|16.1|        15|       1|\n",
      "|998166|2019| 03| 20|34.8|        12|       1|\n",
      "|998252|2019| 04| 18|44.7|        11|       2|\n",
      "|949110|2019| 11| 23|54.9|        14|       2|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod_light.withColumn(\"rank_tpm\", F.ntile(2).over(temp_each_year)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/n_tile.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plain Row Numbers using `row_number()`\n",
    "\n",
    "Given an ordered window, `row_number()` will give an increasing rank (1, 2, 3, . . .) regardless of the ties (the row number of tied records is nondeterministic, so if you need to have reproducible results, make sure you order each window so that there are no ties). This is identical to indexing each window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+--------+\n",
      "|   stn|year| mo| da|temp|count_temp|rank_tpm|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|994979|2017| 12| 11|21.3|        21|       1|\n",
      "|998012|2017| 03| 02|31.4|        24|       2|\n",
      "|719200|2017| 10| 09|60.5|        11|       3|\n",
      "|996470|2018| 03| 12|55.6|        12|       1|\n",
      "|076470|2018| 06| 07|65.0|        24|       2|\n",
      "|917350|2018| 04| 21|82.6|         9|       3|\n",
      "|041680|2019| 02| 19|16.1|        15|       1|\n",
      "|998166|2019| 03| 20|34.8|        12|       2|\n",
      "|998252|2019| 04| 18|44.7|        11|       3|\n",
      "|949110|2019| 11| 23|54.9|        14|       4|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod_light.withColumn(\n",
    "    \"rank_tpm\", F.row_number().over(temp_each_year)\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Losers First: Ordering Your WindowSpec using `orderBy()`\n",
    "\n",
    "Finally, what if we want to reverse the order of our window? Unlike the `orderBy()` method on the data frame, the `orderBy()` method on a window does not have an ascending parameter we can use. We need to resort to the `desc()` method on the\n",
    "Column object directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+----------+\n",
      "|   stn|year| mo| da|temp|count_temp|row_number|\n",
      "+------+----+---+---+----+----------+----------+\n",
      "|041680|2019| 02| 19|16.1|        15|         1|\n",
      "|998012|2017| 03| 02|31.4|        24|         1|\n",
      "|996470|2018| 03| 12|55.6|        12|         2|\n",
      "|998166|2019| 03| 20|34.8|        12|         3|\n",
      "|998252|2019| 04| 18|44.7|        11|         1|\n",
      "|917350|2018| 04| 21|82.6|         9|         2|\n",
      "|076470|2018| 06| 07|65.0|        24|         1|\n",
      "|719200|2017| 10| 09|60.5|        11|         1|\n",
      "|949110|2019| 11| 23|54.9|        14|         1|\n",
      "|994979|2017| 12| 11|21.3|        21|         1|\n",
      "+------+----+---+---+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_per_month_desc = Window.partitionBy(\"mo\").orderBy(\n",
    "    F.col(\"count_temp\").desc()\n",
    ")\n",
    "\n",
    "gsod_light.withColumn(\n",
    "    \"row_number\", F.row_number().over(temp_per_month_desc)\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytic Functions: Looking back and ahead\n",
    "\n",
    "Being able to look at a previous or following record unlocks a lot of functionality when building a time series feature. For instance, when doing modeling on time series data, one of the most important features are the observations in the past. Analytic window functions are by far the easiest\n",
    "way to do this.\n",
    "\n",
    "#### Access The Records Before or After using `lag()` and `lead()`\n",
    "\n",
    "The two most important functions in the analytic functions family are `lag(col, n=1, default=None)` and `lead(col, n=1, default=None)`, which will give you the value of the `col` column of the `n`-th record before and after the record you’re over, respectively. If the record, offset by the lag/lead, falls beyond the boundaries of the window, Spark will default to `default`. To avoid `null` values, pass a value to the optional parameter `default`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+-------------+---------------+\n",
      "|   stn|year| mo| da|temp|count_temp|previous_temp|previous_temp_2|\n",
      "+------+----+---+---+----+----------+-------------+---------------+\n",
      "|994979|2017| 12| 11|21.3|        21|         null|           null|\n",
      "|998012|2017| 03| 02|31.4|        24|         21.3|           null|\n",
      "|719200|2017| 10| 09|60.5|        11|         31.4|           21.3|\n",
      "|996470|2018| 03| 12|55.6|        12|         null|           null|\n",
      "|076470|2018| 06| 07|65.0|        24|         55.6|           null|\n",
      "|917350|2018| 04| 21|82.6|         9|         65.0|           55.6|\n",
      "|041680|2019| 02| 19|16.1|        15|         null|           null|\n",
      "|998166|2019| 03| 20|34.8|        12|         16.1|           null|\n",
      "|998252|2019| 04| 18|44.7|        11|         34.8|           16.1|\n",
      "|949110|2019| 11| 23|54.9|        14|         44.7|           34.8|\n",
      "+------+----+---+---+----+----------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod_light.withColumn(\n",
    "    \"previous_temp\", F.lag(\"temp\").over(temp_each_year)\n",
    ").withColumn(\n",
    "    \"previous_temp_2\", F.lag(\"temp\", 2).over(temp_each_year)\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cumulative Distribution Of the Records uing `cume_dist()`\n",
    "\n",
    "The last analytical function we cover is `cume_dist()`, and it is similar to `percent_rank()`. `cume_dist()`, as its name indicates, provides a cumulative distribution (in the statistical sense of the term) rather than a ranking (where `percent_rank()` shines).\n",
    "\n",
    "$$\\frac{\\# \\text{records with a lower or equal value than the current one}}{\\# \\text{of records in the window}}$$\n",
    "\n",
    "`cume_dist()` is an analytic function. It provides the cumulative density function `F(x)` for the records in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "|   stn|year| mo| da|temp|count_temp|      percent_rank|         cume_dist|\n",
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "|994979|2017| 12| 11|21.3|        21|               0.0|0.3333333333333333|\n",
      "|998012|2017| 03| 02|31.4|        24|               0.5|0.6666666666666666|\n",
      "|719200|2017| 10| 09|60.5|        11|               1.0|               1.0|\n",
      "|996470|2018| 03| 12|55.6|        12|               0.0|0.3333333333333333|\n",
      "|076470|2018| 06| 07|65.0|        24|               0.5|0.6666666666666666|\n",
      "|917350|2018| 04| 21|82.6|         9|               1.0|               1.0|\n",
      "|041680|2019| 02| 19|16.1|        15|               0.0|              0.25|\n",
      "|998166|2019| 03| 20|34.8|        12|0.3333333333333333|               0.5|\n",
      "|998252|2019| 04| 18|44.7|        11|0.6666666666666666|              0.75|\n",
      "|949110|2019| 11| 23|54.9|        14|               1.0|               1.0|\n",
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod_light.withColumn(\n",
    "    \"percent_rank\", F.percent_rank().over(temp_each_year)\n",
    ").withColumn(\"cume_dist\", F.cume_dist().over(temp_each_year)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using row and range boundaries\n",
    "\n",
    "We introduce how to build static, growing, and unbounded windows based on rows and ranges.\n",
    "\n",
    "Let's start by applying an average computation over two windows identically partitioned. The only difference is that the first one is not ordered while the second one is. Surely the order of a window would have no impact on the computation of the average, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "|   stn|year| mo| da|temp|count_temp|            avg_NO|             avg_O|\n",
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "|994979|2017| 12| 11|21.3|        21|37.733333333333334|              21.3|\n",
      "|998012|2017| 03| 02|31.4|        24|37.733333333333334|             26.35|\n",
      "|719200|2017| 10| 09|60.5|        11|37.733333333333334|37.733333333333334|\n",
      "|996470|2018| 03| 12|55.6|        12| 67.73333333333333|              55.6|\n",
      "|076470|2018| 06| 07|65.0|        24| 67.73333333333333|              60.3|\n",
      "|917350|2018| 04| 21|82.6|         9| 67.73333333333333| 67.73333333333333|\n",
      "|041680|2019| 02| 19|16.1|        15|            37.625|              16.1|\n",
      "|998166|2019| 03| 20|34.8|        12|            37.625|             25.45|\n",
      "|998252|2019| 04| 18|44.7|        11|            37.625|31.866666666666664|\n",
      "|949110|2019| 11| 23|54.9|        14|            37.625|            37.625|\n",
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "not_ordered = Window.partitionBy(\"year\")\n",
    "\n",
    "ordered = not_ordered.orderBy(\"temp\")\n",
    "\n",
    "gsod_light.withColumn(\n",
    "    \"avg_NO\", F.avg(\"temp\").over(not_ordered)\n",
    ").withColumn(\"avg_O\", F.avg(\"temp\").over(ordered)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                     ^                      ^\n",
    "            _________________________|   ___________________|\n",
    "            All good: average is cons-   Some odd stuff is happening\n",
    "            -istent across each window   It looks like each window grows,\n",
    "                                         record by record, so the avg changes\n",
    "\n",
    "Something with the ordering of a window messes up the computation. The official Spark API documentation informs us that when ordering is not defined, an unbounded window frame (`rowFrame`, `unboundedPreceding`, `unboundedFollowing`) is used by default. When ordering is defined, a growing window frame (`rangeFrame`, `unboundedPreceding`, `currentRow`) is used by default.\n",
    "\n",
    "We need to understand the types of window frames we build and how they are used. We start by introducing the different _frame sizes_ (static versus growing versus unbounded) and how to reason about them before adding the second dimension, the _frame type_ (range versus rows). At the end of this section, the explanation for the previous code will make perfect sense."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting, window style: Static, growing, unbounded\n",
    "\n",
    "We will cover the boundaries of a window, something we call a _window frame_. We will introduce record based boundaries which will provide an incredible new layer of flexibility when using window functions, as it controls the scope of visibility of a record within the window. We'll be able to create window functions that only look in the past and avoid feature leakage when working with time series.\n",
    "\n",
    "Let’s take a visual of a window: when a function is applied to it, a window spec partitions a data frame based on one or more column values and then (potentially) orders them. Spark also provides the `rowsBetween()` and `rangeBetween()` methods to create window frame boundaries. \n",
    "\n",
    "<img src=\"images/window_frame.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_ordered = Window.partitionBy(\"year\").rowsBetween(\n",
    "    Window.unboundedPreceding, Window.unboundedFollowing\n",
    ")\n",
    "\n",
    "ordered = not_ordered.orderBy(\"temp\").rangeBetween(\n",
    "    Window.unboundedPreceding, Window.currentRow\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explicitly added the boundaries that Spark assumes when none are provided. This means that not_ordered and ordered will provide the same results whether we\n",
    "define the boundaries or not. The ordered window spec is bounded by range, not rows, but for our data frame, it works just the same. \n",
    "\n",
    "Because the window used in the computation of avg_NO is unbounded, meaning that it spans from the first to the last record of the window, the average is consistent across the whole window. The one used in the computation of avg_O is growing on the left, meaning that the right record is bounded to the currentRow, where the left record is set at the first value of the window. As you move from one record to\n",
    "the next, the average is over more and more values. The average of the last record of the window contains all the values (because currentRow is the last record of the\n",
    "window). A static window frame is nothing more than a window where both records are bounded relative to the current row; for example, rowsBetween(-1, 1) for a\n",
    "window that contains the current row, the record immediately preceding, and the record immediately following"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What you are Vs. Where you are: Range Vs Rows\n",
    "\n",
    "Working with ranges is useful when working with dates and time, as you may want to gather windows based on time intervals that are different than the primary measure. As an example, the gsod data frame collects daily temperature information. What happens if we want to compare this temperature to the average of the previous month? Months have 28, 29, 30, or 31 days. This is where ranges get useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+----------+----------+\n",
      "|   stn|year| mo| da|temp|count_temp|        dt|    dt_num|\n",
      "+------+----+---+---+----+----------+----------+----------+\n",
      "|994979|2019| 12| 11|21.3|        21|2019-12-11|1576018800|\n",
      "|998012|2019| 03| 02|31.4|        24|2019-03-02|1551481200|\n",
      "|719200|2019| 10| 09|60.5|        11|2019-10-09|1570572000|\n",
      "|917350|2019| 04| 21|82.6|         9|2019-04-21|1555797600|\n",
      "|076470|2019| 06| 07|65.0|        24|2019-06-07|1559858400|\n",
      "|996470|2019| 03| 12|55.6|        12|2019-03-12|1552345200|\n",
      "|041680|2019| 02| 19|16.1|        15|2019-02-19|1550530800|\n",
      "|949110|2019| 11| 23|54.9|        14|2019-11-23|1574463600|\n",
      "|998252|2019| 04| 18|44.7|        11|2019-04-18|1555538400|\n",
      "|998166|2019| 03| 20|34.8|        12|2019-03-20|1553036400|\n",
      "+------+----+---+---+----+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod_light_p = (\n",
    "    gsod_light.withColumn(\"year\", F.lit(2019))\n",
    "    .withColumn(\n",
    "        \"dt\",\n",
    "        F.to_date(\n",
    "            F.concat_ws(\"-\", F.col(\"year\"), F.col(\"mo\"), F.col(\"da\"))\n",
    "        ),\n",
    "    )\n",
    "    .withColumn(\"dt_num\", F.unix_timestamp(\"dt\"))\n",
    ")\n",
    "\n",
    "gsod_light_p.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple range window, let’s compute the average of the temperatures recorded one month before and after a given day. Because our numerical date is in seconds, I’ll keep things simple and say that 1 month = 30 days = 720 hours = 43,200 minutes = 2,592,000 seconds.\n",
    "\n",
    "<img src=\"images/window_range.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+----------+----------+------------------+\n",
      "|   stn|year| mo| da|temp|count_temp|        dt|    dt_num|         avg_count|\n",
      "+------+----+---+---+----+----------+----------+----------+------------------+\n",
      "|041680|2019| 02| 19|16.1|        15|2019-02-19|1550530800|             15.75|\n",
      "|998012|2019| 03| 02|31.4|        24|2019-03-02|1551481200|             15.75|\n",
      "|996470|2019| 03| 12|55.6|        12|2019-03-12|1552345200|             15.75|\n",
      "|998166|2019| 03| 20|34.8|        12|2019-03-20|1553036400|              14.8|\n",
      "|998252|2019| 04| 18|44.7|        11|2019-04-18|1555538400|10.666666666666666|\n",
      "|917350|2019| 04| 21|82.6|         9|2019-04-21|1555797600|              10.0|\n",
      "|076470|2019| 06| 07|65.0|        24|2019-06-07|1559858400|              24.0|\n",
      "|719200|2019| 10| 09|60.5|        11|2019-10-09|1570572000|              11.0|\n",
      "|949110|2019| 11| 23|54.9|        14|2019-11-23|1574463600|              17.5|\n",
      "|994979|2019| 12| 11|21.3|        21|2019-12-11|1576018800|              17.5|\n",
      "+------+----+---+---+----+----------+----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ONE_MONTH_ISH = 30 * 60 * 60 * 24 # or 2_592_000 seconds\n",
    "\n",
    "one_month_ish_before_and_after = (\n",
    "    Window.partitionBy(\"year\")\n",
    "    .orderBy(\"dt_num\")\n",
    "    .rangeBetween(-ONE_MONTH_ISH, ONE_MONTH_ISH)\n",
    ")\n",
    "\n",
    "gsod_light_p.withColumn(\n",
    "    \"avg_count\", F.avg(\"count_temp\").over(one_month_ish_before_and_after)\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/window_types.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going full circle: Using UDFs within windows\n",
    "\n",
    "Let's see how to use UDFs within windows, using UDFs and split-apply-combine paradigm we saw in ![Previous chapter](./9_Pandas_UDF.ipynb). The recipe for applying a pandas UDF is very simple:\n",
    "1. We need to use a Series to Scalar UDF (or a group aggregate UDF). PySpark will apply the UDF to every window (once per record) and put the (scalar) value as a result.\n",
    "2. A UDF over _unbounded window frames_ is only supported by Spark 2.4 and above.\n",
    "3. A UDF over _bounded window frames_ is only supported by Spark 3.0 and above.\n",
    "\n",
    "This simple `median` function computes the median of a pandas `Series`. Then we apply it twice to the `gsod_light` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+-----------+-------------+\n",
      "|   stn|year| mo| da|temp|count_temp|median_temp|median_temp_g|\n",
      "+------+----+---+---+----+----------+-----------+-------------+\n",
      "|998012|2017| 03| 02|31.4|        24|       31.4|         31.4|\n",
      "|719200|2017| 10| 09|60.5|        11|       31.4|        45.95|\n",
      "|994979|2017| 12| 11|21.3|        21|       31.4|         31.4|\n",
      "|996470|2018| 03| 12|55.6|        12|       65.0|         55.6|\n",
      "|917350|2018| 04| 21|82.6|         9|       65.0|         69.1|\n",
      "|076470|2018| 06| 07|65.0|        24|       65.0|         65.0|\n",
      "|041680|2019| 02| 19|16.1|        15|      39.75|         16.1|\n",
      "|998166|2019| 03| 20|34.8|        12|      39.75|        25.45|\n",
      "|998252|2019| 04| 18|44.7|        11|      39.75|         34.8|\n",
      "|949110|2019| 11| 23|54.9|        14|      39.75|        39.75|\n",
      "+------+----+---+---+----+----------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Spark 2.4, use the following\n",
    "# @F.pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "\n",
    "@F.pandas_udf(\"double\")\n",
    "def median(vals: pd.Series) -> float:\n",
    "    return vals.median()\n",
    "\n",
    "gsod_light.withColumn(\n",
    "    \"median_temp\", median(\"temp\").over(Window.partitionBy(\"year\"))\n",
    ").withColumn(\n",
    "    \"median_temp_g\",\n",
    "    median(\"temp\").over(\n",
    "        Window.partitionBy(\"year\").orderBy(\"mo\", \"da\")\n",
    "    ),\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main steps to a successful window function\n",
    "\n",
    "If you are stumped on how to perform a certain transformation, always remember the basic parameters of using a window function:\n",
    "1. What kind of operation do I want to perform? Summarize, rank, or look ahead/behind.\n",
    "2. How do I need to construct my window? Should it be bounded or unbounded? Do I need every record to have the same window value (unbounded), or should the answer depend on where the record fits within the window (bounded)? When bounding a window frame, you most often want to order it as well.\n",
    "3. For bounded windows, do you want the window frame to be set according to the position of the record (row based) or the value of the record (range based)?\n",
    "4. Finally, remember that a window function does not make your data frame special. After your function is applied, you can filter, group by, and even apply another, completely different, window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05de79a9bc4beb95fb2b07d395d8e3fe55e6d8497bda19361fbfb16b724883dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
